#+BIND: org-export-use-babel nil
#+TITLE: A comparison between regular and Expected SARSA
#+AUTHOR: Luc Weytingh, Paul Lodder, Pim Meerdink, Jeroen Jagt
#+EMAIL: University of Amsterdam, University of Amsterdam, University of Amsterdam, University of Amsterdam
#+DATE: \today
#+LATEX: \setlength\parindent{0pt}
#+LaTeX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage[margin=0.8in]{geometry}
#+LATEX_HEADER_EXTRA:  \usepackage{mdframed}
#+LATEX_HEADER_EXTRA: \BeforeBeginEnvironment{minted}{\begin{mdframed}}
#+LATEX_HEADER_EXTRA: \AfterEndEnvironment{minted}{\end{mdframed}}
#+MACRO: NEWLINE @@latex:\\@@ @@html:<br>@@
#+PROPERTY: header-args :exports both :session blogpost :cache :results value
#+OPTIONS: ^:nil
#+LATEX_COMPILER: pdflatex

* Introduction

Humans -- and animals -- do a great deal of learning throughout their life. We
learn things about the environment we're in by our experiences we have in it,
and their outcomes. Nowadays, many researchers in A.I. work on methods through
which algorithms can learn from experiences in an environment to construct a
strategy, often called a policy, on how best to act in that environment. This
field of research is called Reinforcement Learning (RL).

In RL, an algorithm acts in the following way: from some state $S$ (its current
position in the world), it selects an action $A$ (something it can /do/ from
that position), which results in a reward $R$, and ending up in a next state
$S'$. It would then pick a new action $A'$, get another reward $R'$, and end up
in another state, all the way until the end goal is reached[fn:: In this
blogpost, we only consider episodic tasks, which are tasks which always have an
ending.]. Based on the current state $S$, the algorithm decides which action
$A$ to pick according to a policy function $\pi(S) = A$. When learning from
experiences, the goal is to find some policy $\pi$ through which, if the
algorithm were to use that, the total reward is maximized ($R + R' + R'' +
\dots$).

In this blogpost, we highlight the difference between /Sarsa/ and /Expected
Sarsa/, two algorithms for finding an optimal policy.

#paul: i would leave this out, on-policy vs off-policy
# When we are getting experiences, we need to use some policy as well. This can
# either be the very same policy we are improving with those experiences
# (on-policy), or any different strategy/policy we would like to use
# (off-policy). In this blog post, we want to highlight the differences between
# one popular RL technique called SARSA, which is on-policy, and a variant on
# it called Expected SARSA (ESARSA), which is off-policy.


#paul: Perhaps we need to introduce the concept of Q functions and policies here
* Theory
Both algorithms find the best policy by learning an optimal /action-value/
function, or /Q-function/. A Q-function aims to answer the following question
for each state $S$ and possible action $A$: how much reward will the agent
receive if it takes action $A$ from state $S$?

Both in SARSA and ESARSA, we let an agent interact with our environment to
generate experience. Using this experience, we update our Q function, and the
agent bases its policy on the most updated Q function. Hence, as the Q function
improves, so does the behaviour of our agent.

However, by letting our agent base its behaviour on the very Q function we are
trying to optimize, we may get a form of "tunnel vision", where the agent will
only keep performing actions it already thinks are good. A potential result of
this is that certain state-action pairs are never visited. A common approach to
tackle this is by letting the agent have an /\epsilon-greedy policy/ w.r.t. the Q
function: 1-\epsilon of the time, the agent behaves according to what it thinks is
best, in the remaining \epsilon of the time it picks a random action. By doing this,
we make sure the agent maintains some exploratory behaviour, while still mainly
adhering to what our algorithm currently thinks is optimal behaviour. [perhaps
visualize this policy].

Hence, both SARSA and ESARSA learn from an agent that is largely acting
according to their current version of the Q-function. The crucial difference
that distinguishes SARSA from ESARSA, is the way in which we update our Q
function. In SARSA, our update rule is:
# #+begin_export latex
\begin{equation}
     Q(S_{t}, A_{t}) = Q(S_{t}, A_{t}) + \alpha (R_{t+1}+\gamma Q(S_{t+1}, A_{t+1})-Q(S_{t}, A_{t}))
\end{equation}
# #+end_export
whereas in ESARSA, it is:
# #+begin_export latex
\begin{equation}
Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \alpha (r_{t+1}+\gamma \sum_{a} \pi (a | s_{t+1}) Q(s_{t+1}, a)-Q(s_{t}, a_{t}))
\end{equation}
#+end_export\\
The important difference here is that SARSA's update rules uses the actual
trajectory, i.e. the actual action the agent picked at the next state (which
may have been a random, terrible action). In contrast, ESARSA uses the expected
reward according to the current policy, i.e. weighting the rewards for each of
the possible next actions by the probability of taking that action.

As you can already see from the complexity of the update equations, ESARSA
requires more computation per update step than SARSA. On the flipside, however,
ESARSA is expected to require fewer update steps.


#   assigns a numerical value to each state $S$ and
# action $A$ that can be taken from it, we find some value for the pair $(S, A)$
# which reflects how rewarding it is to take that action. We can then derive a
# policy based on the resulting action-value function by e.g. (almost) always
# picking the action with the largest associated value. We use this policy to get
# more experiences, and with every new experience, we update the value of $(S,
# A)$ based on its reward $R$ and /the value of the *best* action $A'$ we can
# take in the state $S'$/ (in which we've ended up after taking $A$).


# ESARSA is similar, but instead of taking the value of the best action, we take
# the average of the values of all actions we could take from that next state
# $S'$. In doing so, according to the theory, ESARSA should take a little bit
# more (computational) effort in every learning experience than SARSA does, but
# it should then require fewer experiences to learn a policy which performs as
# well as the policy SARSA would produce with more experiences.


* Hypothesis
TODO: not really happy with the placement of this part but i think it

We want to investigate whether the theoretical claims about computational
complexity and sample efficiency actually hold in practice. To do so, we apply
SARSA and ESARSA on the same tasks, give them the same number of episodes to
learn a policy, and measure how long the whole process takes, as well as the
performance of the policy as it is being learnt. We measure how long the
learning process takes in two ways. First, we simply record the actual time
taken in seconds. Second, we also record the amount of update steps taken (how
many samples the algorithm has observed). We will plot both against the return
(how much reward the algorithm receives). In these plots we expect to observe
that ESARSA learns faster when we observe the amount of samples it needs
(ESARSA draws more reliable information from each sample due to the expectation
it computes). However, we expect the opposite to be true (i.e. SARSA will be
faster than ESARSA) when we observe the return as a function of the time
elapsed, as the ESARSA update is more expensive it will need more computational
time to process each sample.

* Method

** Environments



To test the theoretical claims, we need tasks to compare the results of SARSA
and ESARSA. Luckily for us, we don't have to construct these tasks
ourselves. Gym is an open-source toolkit for training and developing RL
algorithms, it contains tasks that range from learning agents to walk, to
playing games like pong or pinball. In addition, it provides methods for
interacting with an environment that are identical for all tasks, making it
easier for us to run experiments on a range of tasks using the same code.

Since we are comparing two approaches that are both designed to have a finite
amount of states, we want the state space to be non-continuous. We often refer
to tasks with these kind of state-spaces as /tabular/ tasks. For our
experiments, we selected 4 tabular tasks from the gym library:

1. Copy-V0, simple, toy
2. Taxi-v2, fully observable
2. Blackjack-v0, partial observability
3. FrozenLake8x8-v0: stochastic

TODO:: write about why we choose these



** Seeds
Many of the environments that we run our algorithms on are stochastic (that is
to say: they involve some randomness). This means that we can expect slightly
different results every time we interact with the environment, even if our
policy is equal. Given that it is important for the research we perform to be
reproducible, we seed our environments using some constants. A seed ensures
that we can expect the /same/ randomness every time we seed with the same
integer. To ensure that we are not overfitting to a particular seed, we also
make sure to execute multiple runs with different seeds whenever we train a
algorithm in an environment.


** Hypeparameters
As with most AI techniques, an important design choice we need to think about
is choosing the right hyperparameters. First, we will look at $\alpha$,
representing the learning rate in the SARSA and ESARSA equations. It may seem
tempting at first to choose a reasonable value, say 0.1, for $\alpha$ and keep
it constant across all experiments for SARSA and ESARSA. After all, as long as
we keep it constant, the comparison is fair, right? Well, not quite. SARSA and
ESARSA likely have different a optimal $\alpha$, and this is related to the
variance of their updates. In short, since we are calculating an expectation
over the possible next states and their Q in ESARSA, and simply sampling one in
SARSA (the one we chose), the SARSA updates will have much higher variance
thatn the ESARSA updates. This means that SARSA has to be more careful, and
take smaller steps with each update, than ESARSA. This also ties into the
theory of the computational time/sample efficiency tradeoff. Each update that
ESARSA does will be more expensive, but it can then afford to take a larger
step in the direction it computed, than SARSA. Thus, in order to facilitate a
fair comparison between both algorithms, its important to make sure both can
use their strengths to the fullest extent, which requires setting different
$\alpha$s. To this end, we set up a parameter search which finds the optimal
alpha for each algorithm, environment combination, and use that when running
our experiments.

The other hyperparameter that is present in both of our algorithms is the
discount factor $\gamma$, it represents how much our algorithms cares about
future rewards, as opposed to immediate rewards. Unlike $\alpha$, its optimal
value is not influenced by the variance of the updates of the algorithms,
besides this it is present in both update rules in the same place, being
multiplied by the estimate of the reward of the next state. For these reasons,
we can safely keep it at a  constant value for both of our algorithms, while
still assuring fair comparison.

* Results
#+CAPTION: Mean episode return SARSA and Expected SAESA for the CliffWalking-v0 env
#+NAME:   fig:cliffwalk
[[./src/CliffWalking-v0.png]]
* Conclusion


- meer uitleg graphs
- hyperaparms
- why choos env
