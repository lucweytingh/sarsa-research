#+BIND: org-export-use-babel nil
#+TITLE: A comparison between regular and Expected SARSA
#+AUTHOR: Luc Weytingh, Paul Lodder, Pim Meerdink, Jeroen Jagt
#+EMAIL: University of Amsterdam, University of Amsterdam, University of Amsterdam, University of Amsterdam
#+DATE: \today
#+LATEX: \setlength\parindent{20pt}
#+LaTeX_HEADER: \usepackage{minted}
#+LaTeX_HEADER: \usepackage{mathpazo}
#+LATEX_HEADER: \usepackage[margin=0.3in]{geometry}
#+LATEX_HEADER_EXTRA:  \usepackage{mdframed}
#+LATEX_HEADER_EXTRA: \BeforeBeginEnvironment{minted}{\begin{mdframed}}
#+LATEX_HEADER_EXTRA: \AfterEndEnvironment{minted}{\end{mdframed}}
#+LATEX_CLASS_OPTIONS: [9pt]
#+MACRO: NEWLINE @@latex:\\@@ @@html:<br>@@
#+PROPERTY: header-args :exports both :session blogpost :cache :results value
#+OPTIONS: ^:nil toc:nil
#+LATEX_COMPILER: pdflatex
#+BIBLIOGRAPHY: refs plain


* Introduction

Humans -- and animals -- do a great deal of learning throughout their life. We
learn things about the environment we're in by our experiences we have in it,
and their outcomes. Nowadays, many researchers in A.I. work on methods through
which algorithms can learn from experiences in an environment to construct a
strategy, often called a policy, on how best to act in that environment. This
field of research is called Reinforcement Learning (RL).

In RL, an algorithm acts in the following way: from some state $S$ (its current
position in the world), it selects an action $A$ (something it can /do/ from
that position), which results in a reward $R$, and in ending up in a next state
$S'$. It would then pick a new action $A'$, get another reward $R'$, and end up
in another state, all the way until the end goal is reached[fn:: In this
blogpost, we only consider episodic tasks, which are tasks which always have an
ending.]. Based on the current state $S$, the algorithm decides which action
$A$ to pick according to a policy function $\pi(S) = A$. When learning from
experiences, the goal is to find some policy $\pi$ through which, if the
algorithm were to use that, the total reward is maximized ($R + R' + R'' +
\dots$). The policy can be anything, but one popular class of approaches is
based on an /action-value/ function (or /Q-function/), which represents how
/valuable/ or /rewarding/ it is to take an action from some state. The policy
function could then just be defined to pick the most valuable actions.

In this blogpost, we highlight the difference between /SARSA/ and /Expected
SARSA/ (ESARSA), two algorithms for finding an optimal policy. <<jpj: we need
some additional text here.>>

# paul: i would leave this out, on-policy vs off-policy
# When we are getting experiences, we need to use some policy as well. This can
# either be the very same policy we are improving with those experiences
# (on-policy), or any different strategy/policy we would like to use
# (off-policy). In this blog post, we want to highlight the differences between
# one popular RL technique called SARSA, which is on-policy, and a variant on
# it called Expected SARSA (ESARSA), which is off-policy.


# paul: Perhaps we need to introduce the concept of Q-functions and policies
# here


** SARSA or ESARSA: what's the difference?

# Both algorithms find the best policy by learning an optimal /action-value/
# function, or /Q-function/. A Q-function aims to answer the following question
# for each state $S$ and possible action $A$: how much reward will the agent
# receive if it takes action $A$ from state $S$?

Of course, before training, we do not know the Q-function, i.e. how valuable
each action is [fn:: If we would know this, we would not really need to learn
anything.]. Both in SARSA and ESARSA, we let an agent interact with our
environment to generate experience. Using this experience, we update the
Q-function, and since the agent's policy is based on the Q-function, so is the
policy (and behaviour) of the agent.

However, by letting our agent base its behaviour on the very Q-function we are
trying to optimize, we may get a form of ``tunnel vision'', where the agent
will only choose to perform actions it already thinks are good. A potential
result of this is that certain actions might never be picked, even if in
reality -- unbeknownst to us -- that action would be more rewarding! A common
way to tackle this problem is to use a policy through which the agent behaves
according to what it thinks most of the time, but /sometimes/, picks a random
action. Formally, this is known as an /\epsilon-greedy policy/, where $\epsilon
\in [0, 1]$ indicates how often a random action is chosen. This ensures that
the agent will still explore from time to time, but not too often (because then
we'd lose out on rewards).

# A common approach to tackle this is by letting the agent have an
# /\epsilon-greedy policy/ w.r.t. the Q-function: 1-\epsilon of the time, the
# agent behaves according to what it thinks is best, in the remaining \epsilon
# of the time it picks a random action. In doing so, we make sure the agent
# maintains some exploratory behaviour, while still mainly adhering to what our
# algorithm currently thinks is optimal behaviour. [perhaps visualize this
# policy].

Hence, both SARSA and ESARSA learn from an agent that is largely acting
according to their current version of the Q-function. The crucial difference
which distinguishes SARSA from ESARSA is in the rule through which our
Q-function is updated (based on an experience). In SARSA, the update rule is:

\begin{equation}
     Q(S_{t}, A_{t}) = Q(S_{t}, A_{t}) + \alpha (R_{t+1}+\gamma Q(S_{t+1}, A_{t+1})-Q(S_{t}, A_{t}))
\end{equation}

whereas in ESARSA, it is:

\begin{equation}
Q(S_{t}, a_{t}) = Q(S_{t}, a_{t}) + \alpha (R_{t+1}+\gamma \sum_{a} \pi (a | S_{t+1}) Q(S_{t+1}, a_{t+1})-Q(s_{t}, a_{t}))
\end{equation}
#+end_export
Note the structural similarity between (1) and (2): both are of the form:\\
$$Q(S_{t},a_{t}) = Q(S_{t},a_{t}) + \alpha(U_{t} - Q(S_{t},A_{t}))$$\\
This update step can be seen as "pushing" $Q(S_{t}, a_{t})$ towards $U_{t}$, which we also
call the /target/.

The difference here is that SARSA's target is based on the taken trajectory,
i.e. the actual action the agent picked at the next state. In contrast, ESARSA
uses the expected reward according to the current policy, i.e. weighting the
rewards for each of the possible next actions by the probability of taking that
action.

Evidently, ESARSA requires more computations per update step than SARSA, and
so, has a higher *computational complexity*. On the flipside, however, ESARSA
is expected to reach better performance in fewer update steps than SARSA,
because the update step is supposed to extract more information from one
experience, and so, has a higher *sample efficiency* than SARSA.

# <<TODO: introduce the idea of "sample" (= "one experience")?>>

#   assigns a numerical value to each state $S$ and
# action $A$ that can be taken from it, we find some value for the pair $(S, A)$
# which reflects how rewarding it is to take that action. We can then derive a
# policy based on the resulting action-value function by e.g. (almost) always
# picking the action with the largest associated value. We use this policy to get
# more experiences, and with every new experience, we update the value of $(S,
# A)$ based on its reward $R$ and /the value of the *best* action $A'$ we can
# take in the state $S'$/ (in which we've ended up after taking $A$).


# ESARSA is similar, but instead of taking the value of the best action, we take
# the average of the values of all actions we could take from that next state
# $S'$. In doing so, according to the theory, ESARSA should take a little bit
# more (computational) effort in every learning experience than SARSA does, but
# it should then require fewer experiences to learn a policy which performs as
# well as the policy SARSA would produce with more experiences.

** The experiment

Of course, if we actually would have to make a choice between SARSA and ESARSA,
we would care more about practical results than theoretical ones. So, the
question is: do the theoretical claims about computational complexity and
sample efficiency actually hold in practice?

In order to investigate this question, let's run an experiment! We'll apply
SARSA and ESARSA on the same tasks, give them the same number of episodes to
learn a policy, and measure how long the whole process takes, as well as the
performance of the policy as it is being learned.

We need to measure how long the learning process takes in two ways. For the
computational complexity, we should record the actual time taken in
seconds. For the sample efficiency, we also record the amount of update steps
taken (how many samples the algorithm has observed). We will plot both against
the return (how much reward the algorithm receives).

# ESARSA draws more reliable information from each sample due to the
# expectation it computes
Of course, since the theory /should/ be correct, our hypothesis is that the
practical experiments support the theoretical claims. In the plots, we expect
to observe that ESARSA learns faster when we observe the amount of samples it
needs, and thus, has a higher sample efficiency. Likewise, we expect SARSA to
finish the required number of episodes faster than ESARSA because the latter
has a higher computational complexity (and so, takes longer for every
sample). This means that we expect similar performance when we observe the
speed at which the algorithms learn as a function of the time
elapsed. Dependent on the stochasticity and complexity of the task at hand, we
can expect these differences to be more or less pronounced.

* Method

** Environments
To run these experiments, we'll need some tasks to solve. Luckily for us, we
don't have to construct these tasks ourselves. [[https://gym.openai.com/][=Gym=]] is an open-source toolkit
for training and developing RL algorithms, and contains tasks that range from
learning agents to walk, to playing games like pong or pinball. In addition, it
provides methods for interacting with an environment that are identical for all
tasks, making it easier for us to run experiments on a range of tasks using the
same code.

Since we are comparing two approaches that are both designed to have a finite
amount of states, we want the state space to be non-continuous. We often refer
to tasks with these kind of state-spaces as /tabular/ tasks. For our
experiments, we selected 4 tabular tasks from the gym library: =Copy-v0=,
=Taxi-v2=, =Blackjack-v0=, and =FrozenLake-v1=.

For a fair comparison, the strengths and weaknesses of SARSA and ESARSA have to
be taken into account. Due to the distinct update rules for the Q functions
they estimate, SARSA and ESARSA behave differently in environments with
different amounts of observability, uncertainty and actions. Therefore, the
first two environments (=Copy-v0= and =Taxi-v2=) are fully observable with
deterministic transition functions and the other two (Blackjack-v0 and
FrozenLake-v1) are not fully observable and introduce uncertainty.

To better understand the tasks that our algorithms will be up against, lets
have a closer look at the chosen environments. Copy-v0 is a simple toy problem,
the task is to copy symbols from an input to an output tape. Taxi-v2 introduces
a bit more complexity. It tasks agents with picking up and dropping off a
passenger at particular locations. These two problems should be solvable by
both algorithms quite easily, and we expect to see the theoretical tradeoff
quite clearly.The goal in Blackjack-v0 is to find a winning card playing
strategy against a computer dealer. Blackjack is a simple game as far as card
games go, but a nontrivial task for our RL algorithm, nonetheless. We selected
this environment because +we can become millionaires+ of the partial
observability and uncertainty of the environment. Finally, in FrozenLake-v1,
the agent is tasked with moving across a grid of ice with unobservable holes in
it, which the agent can fall through. What's more, the ice is slippery, this
means that the agent won't always move in the direction it intends to. These
factors combined make the environment partially observable, aswell as
stochastic.


** Seeds

Some of the environments that we run our algorithms on are stochastic (that is
to say: they involve some randomness). Our algorithm is stochastic aswell. This
means that we can expect slightly different results every time we interact with
the environment, even if our policy is equal. Given that it is important for
the research we perform to be reproducible, we seed our environments using some
constants. A seed ensures that we can expect the /same/ randomness every time
we seed with the same integer. To ensure that we are not overfitting to a
particular seed, we also make sure to execute multiple runs with different
seeds whenever we train an algorithm in an environment.


** Hyperparameters

As with most AI techniques, an important design choice we need to think about
is choosing the right hyperparameters. First, we will look at $\alpha$,
representing the learning rate in the SARSA and ESARSA equations. It may seem
tempting at first to choose a reasonable value, say 0.1, for $\alpha$ and keep
it constant across all experiments for SARSA and ESARSA. After all, as long as
we keep it constant, the comparison is fair, right? Well, not quite. SARSA and
ESARSA likely have a different optimal $\alpha$, and this is related to the
variance of their updates. In short, since we are calculating an expectation
over the possible next states and their Q-values in ESARSA, and simply sampling
one in SARSA (the one we chose), the SARSA updates will have much higher
variance than the ESARSA updates. This means that SARSA has to be more careful,
and take smaller steps with each update, than ESARSA. This also ties into the
theory of the computational time/sample efficiency tradeoff. Each update that
ESARSA does will be more expensive, but it can then afford to take a larger
step in the direction it computed, than SARSA. Thus, in order to facilitate a
fair comparison between both algorithms, its important to make sure both can
use their strengths to the fullest extent, which requires setting a different
$\alpha$. To this end, we set up a parameter search which finds the optimal
alpha for each algorithm, environment combination, and use that when running
our experiments.

The other hyperparameter that is present in both of our algorithms is the
discount factor $\gamma$, it represents how much our algorithms care about
rewards in the future, as opposed to immediate rewards. Unlike $\alpha$, its
optimal value is not influenced by the variance of the updates of the
algorithms, besides this it is present in both update rules in the same place,
being multiplied by the estimate of the reward of the next state. Besides this,
it is often viewed as an attribute of the problem (how much do we care about
future rewards) as opposed to the algorithm. For these reasons, we can safely
keep it at a constant value for both of our algorithms, while still assuring
fair comparison.

* Results
** =Copy=
#+CAPTION: Mean episode return over SARSA and expected SARSA for the Copy-v0 env
#+NAME:   fig:copy
[[./plots/Copy-v0 (50 runs).png]]


When observing the plots in Figure [[fig:copy]], we see that notably ESARSA
outperforms SARSA, attaining a much higher mean episodic return. On the left
hand side, we see that the rate at which the algorithms learn are similar, with
SARSA learning slightly more per update 2500 updates, which is not in line with
our expectations. This advantage quickly disappears, though, as ESARSA
continues to rise to a mean episodic return of 5 and a SARSA reaches no higher
than 2.5. On the right hand side, we see a similar pattern, SARSA learns and
finishes significantly faster, but fails to reach the same mean episodic return
as ESARSA. There is a high amount of variance, and contrary to our expectation
the variance is higher for ESARSA, than SARSA.

** =Taxi=
#+CAPTION: Mean episode return over SARSA and expected SARSA for the Taxi-v0 env
#+NAME:   fig:taxi
[[./plots/Taxi-v3 (50 runs).png]]

Figure [[fig:taxi]] displays the learning curves of SARSA and ESARSA for the
Taxi-v3 environment.Both reach a mean episodic return of approximately 0,
meaning they learn the environment fully. As expected, in terms of number of
updates expected ESARSA outperforms SARSA, taking about half the number of
updates as SARSA to reach the same level of performance.

In terms of computational time required, then, ESARSA also outperforms
SARSA. This is contrary to our initial expectations, but can be explained by
the fact that the episodes of well-performing agents in the Taxi environment
are shorter, as they drop off their passengers efficiently. In this case, this
meant that ESARSA quickly learned to end its episodes quickly, and also
required less update steps in total (which we can see on the left hand
side). If we look at the amount of time per update step, SARSA is still faster
than ESARSA (taking about the same amount of time for 1.5 times more
updates. Both algorithms have little variance across runs.

** =Blackjack=
#+CAPTION: Mean episode return over SARSA and expected SARSA for the Blackjack-v0 env
#+NAME:   fig:blackjack
[[./plots/Blackjack-v0 (5000 ep, 50 runs).png]]

In the blackjack environment both algorithms attain relatively little increase
in the mean episodic return. This is due to the high level of stochasticity,
which means the game is not really "solvable" in the same way that the Taxi
environment is. Even a player that plays perfectly will often lose, but simply
less than a poorly playing player. There is lots of variance during the
training process, which again can be attributed to the large amount of
randomness that is inherent to the game.

Both in terms of the number of updates, and the time, SARSA outperforms ESARSA
in terms of mean episodic return. This could be due to the large amount of
variance inherent to the game, which means that ESARSA cannot exploit the lower
variance updates which it produces. Besides this there are only two actions in
the game making the expectation cheap to compute, also resulting in little
difference in total necessary computation time.


** =Frozenlake=
We can see clearly that within a short number of steps and within a short
amount of time, ESARSA exceeds the performance of SARSA. This could be
attributed to the fact that ESARSA more quickly learns to be risk-averse: it
weighs in the risk of falling off when walking right along the edge of the
cliff. SARSA, in contrast, only looks at the actual path taken, and may
therefore learn the risk of walking along the cliff less quickly. After all it
could walk along the cliff for 9 steps safely and only learn the danger once it
actually falls of (due to e.g. an $\epsilon$-picked action).

Note that we can also observe that SARSA does worse here simply by the fact
that we have way less update steps and time to look at: the SARSA agent simply
fell off more often and quickly!

#+CAPTION: Mean episode return over SARSA and expected SARSA for the Frozenlake-v1 env
#+NAME:   fig:frozenlake
[[./plots/FrozenLake-v1 (50 runs).png]]

* Conclusion


- meer uitleg graphs
- hyperaparms
- why choos env
