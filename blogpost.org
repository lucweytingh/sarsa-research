#+BIND: org-export-use-babel nil
#+TITLE: A comparison between regular and Expected SARSA
#+AUTHOR: Luc Weytingh, Paul Lodder, Pim Meerdink, Jeroen Jagt
#+EMAIL: University of Amsterdam, University of Amsterdam, University of Amsterdam, University of Amsterdam
#+DATE: \today
#+LATEX: \setlength\parindent{0pt}
#+LaTeX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage[margin=0.8in]{geometry}
#+LATEX_HEADER_EXTRA:  \usepackage{mdframed}
#+LATEX_HEADER_EXTRA: \BeforeBeginEnvironment{minted}{\begin{mdframed}}
#+LATEX_HEADER_EXTRA: \AfterEndEnvironment{minted}{\end{mdframed}}
#+MACRO: NEWLINE @@latex:\\@@ @@html:<br>@@
#+PROPERTY: header-args :exports both :session blogpost :cache :results value
#+OPTIONS: ^:nil
#+LATEX_COMPILER: pdflatex

* Introduction

Humans -- and animals -- do a great deal of learning throughout their life. We
learn things about the environment we're in by our experiences we have in it,
and their outcomes. Nowadays, many researchers in A.I. work on methods through
which computers can learn from experiences in an environment to construct a
strategy, often called a policy, on how best to act in that environment. This
field of research is called Reinforcement Learning (RL).

In RL, a computer is programmed to act in the following way: from some state
$S$ (its current position in the world), it selects an action $A$ (something it
can /do/ from that position), which results in a reward $R$, and ending up in a
next state $S'$. It would then pick a new action $A'$, get another reward $R'$,
and end up in another state, all the way until the end goal is reached[fn:: In
this blogpost, we only consider episodic tasks, which are tasks which always
have an ending.]. Based on the current state $S$, the computer decides which
action $A$ to pick according to a policy function $\pi(S) = A$. When learning
from experiences, the goal is to find some policy $\pi$ through which, if the
computer were to use that, the sum of rewards is maximized ($R + R' + R'' +
\dots$). When we are getting experiences, we need to use some policy as
well. This can either be the very same policy we are improving with those
experiences (on-policy), or any different strategy/policy we would like to use
(off-policy).

In this blog post, we want to highlight the differences between one popular RL
technique called SARSA, which is on-policy, and a variant on it called Expected
SARSA (ESARSA), which is off-policy. The idea behind SARSA is that, for every
state $S$ and some action $A$ that can be taken from it, we find some value to
the pair $(S, A)$ which reflects how rewarding it is to take that action. The
policy then simply is defined as taking the action associated with the largest
value [TODO: VERIFY THIS?]. We use the policy itself to get more experiences,
and with every new experience, we update the value of $(S, A)$ based on its
reward $R$ and /the value of the *best* action $A'$ we can take in the state
$S'$/ (in which we've ended up after taking $A$). ESARSA is similar, but
instead of taking the value of the best action, we take the average of the
values of all actions we could take from that next state $S'$. In doing so,
according to the theory, ESARSA should take a little bit more (computational)
effort in every learning experience than SARSA does, but it should then require
fewer experiences to learn a policy which performs as well as the policy SARSA
would produce with more experiences.

We want to investigate whether those theoretical claims actually hold in
practice. To do so, we apply SARSA and ESARSA on the same tasks, give them the
same number of episodes to learn a policy, and measure how long the whole
process takes, as well as the performance of the policy as it is being
learnt. We expect to see a confirmation of the theory, namely that ESARSA is a
bit slower than SARSA, but does produce a better-performing policy early on in
the learning process than SARSA does.

* Introduction TOO FUCKING LONG                                    :noexport:

Humans -- and animals -- do a great deal of learning throughout their
life. When we move to a new city, where we have never been before, we find
ourselves in an unknown environment. In order to be able to do the things we
want to, we need to learn things about the environment; for instance, /"Where
is the supermarket?"/ The best way to learn such things is to interact with the
environment, experience what happens as you do so, and learn as much as you can
from those experiences! To find a supermarket in that new city, the easiest
thing is to walk through the streets, and when you find one, remember its
location.[fn:: Well, before the internet, at least...] And the more often we
repeat these experiences, the better we learn their outcomes! If we flip a coin
and it lands on heads everytime, the more times it lands heads, the more
certain we become that something is funky with the coin. In other words, the
more often we experience a certain outcome of some action or event, the
stronger our beliefs about that outcome become. The repetition of those
experiences *reinforce* our beliefs about the world. The learning through
repetition of experiences is what we call *Reinforcement Learning* (RL), and we
do it all the time.

Nowadays, it's not just humans and animals who apply Reinforcement Learning,
but computers can do it as well. This can be very useful: for instance, if we
want some (computer-controlled) robots to do a dangerous (or boring!) activity
so that we don't have to, those robots should know how to perform that
activity. We could try to pre-program the computers so that we tell them what
to do in every possible scenario, but this is very tricky, especially if the
environment in which they act is dynamic and unpredictable. Instead, what if we
would program the computer to learn through reinforcement, and then send a
couple pioneer robots out into the environment to collect experiences? That
way, we are sure that, eventually, they can learn from every situation they
encounter (at the cost, perhaps, of a couple robots). Computers could learn how
to assemble cars, how to mine for precious metals, how to play (video)games,
and so on! <<some more examples?>>

Computers can not experience the world in as much detail and nuance as we
humans do <<TODO: WHY?>>, and so, they need some simplified version of the
world to work with. In RL, researchers use three concepts to achieve this
simplification: states, actions, and rewards.

A state (denoted with $S$) is simply a complete description of (the relevant
parts of) the world in which the computer needs to act. If the computer
controls a robot which can walk around, the state might consist of the current
position of the robot, for instance. If the computer is learning to play a
videogame, the state is often just: the current screen. If the computer
controls an airplane, the state would include all kinds of stuff like: the
weight of the plane, the position in space, whether there's clouds around,
whether the wheels are down or not -- basically, all the things of which we,
the programmer of the computer, think can be relevant when making decisions
(without any superfluous information).

An action (denoted with $A$) is simply an action that a computer can take from
any state. If the computer is controlling that robot which can walk around,
then its actions might be to move forward, or to the left, or to the right, or
backwards (or to stand still).

A reward (denoted with $R$) is any number, either negative or positive, which
is rewarded after an action $A$ is taken from a specific state $S$. If we want
the computer to find the treasure in a maze, then the action taken which
directly exposes the treasure might have a very large reward. For instance,
turning left ($A$) at a certain point in the maze ($S$) might have a reward
$R(S, A) = 100$. When the computer is playing a videogame in which the
challenge is to survive as long as possible, then every action taken through
which the player does not die would have a positive reward.

In this blog post we will only discuss /episodic tasks/, which are tasks which
have some kind of end. If the task is to escape a maze, then one episode would
start at the moment in which the computer is 'dropped' into the maze, and would
end when the exit has been reached. To reach the end of an episode, the
computer has to decide to take a sequence of actions. These actions each have a
reward, and so you can /score/ the overall episode, simply by summing the
reward. When the goal is for the computer to escape as quickly as possible,
every action in which it has not escaped yet might have a small negative
reward. Then, episodes at which the computer takes more actions (time) to
escape the maze have a lower score.

In the study of Artificial Intelligence (AI), many efforts have been made to
try and make it possible for RL to be performed by computers, and the field of
RL has a long history. In (almost all of) this research, the problem boils down
to finding some algorithm which can perform the following task: based on a
bunch of experiences, what is the best way for a computer to learn the optimal
strategy? Here, the higher the average score a computer gets when applying the
strategy in the world, the better we say that strategy is.

<<maybe we can put this in a nice special info box>>
By the way, as humans, we also learn in another way, which we are really good
at: we learn from others. For computers, this is not impossible to accomplish,
but we always need RL for computers keep learning from their experiences. So,
we better get good at it!

* Introduction OLD :noexport:

Methods based on Temporal Difference (TD) learning form a central and novel
role within Reinforcement Learning (RL). A big advantage of TD methods over
earlier approaches is that they work in an online setting with no previous
knowledge of the environment dynamics.

Two main approaches can be identified within the TD methods: on-policy and
off-policy methods. On-policy methods are generally simpler methods. They
generate behaviour and update their action-value estimates using the same
policy. To ensure exploration, a random action is chosen according to some
probability $\epsilon$. Due to this random selection of actions, the learned
action-value estimates are not for the optimal policy, but for a near-optimal
policy that explores randomly. Off-policy methods split the behaviour
generation and action-value updates into two, using a target policy to learn
the optimal policy and a behaviour policy to generate behaviour. This generally
results in a more powerful and general policy, at the cost of slower
convergence [TODO: REFERENCE to RL book].

In this blogpost, we compare two TD methods to confirm this [TODO: nagaan
of this goed terugrefereert] theoretical trade-off between compute time and
sample efficiency: SARSA and Expected SARSA. The former uses an on-policy
approach and the latter uses an off-policy approach. [OPTIONAL] Expected SARSA
is very similar to a more widely known and used off-policy TD-method called
Q-learning. The difference lies within the generation of the actions: while
Q-learning takes the maximum valued action to generate behaviour, Expected
SARSA uses the expected value of each action, thereby taking into account the
probability of each action under the current policy. [/OPTIONAL]

We investigate the differences in several non-continuous environments from the
open source gym library [TODO: reference], reporting on the differences in
 compute-time and sample-efficiency and their origin.


# HYPOTHESE

** Hypothesis

* Method

To test the theoretical claims, we need tasks to compare the results of SARSA
and ESARSA. Luckily for us, we don't have to construct these tasks
ourselves. Gym is an open-source toolkit for training and developing RL
algorithms, it contains tasks that range from learning agents to walk, to
playing games like pong or pinball. In addition, it provides methods for
interacting with an environment that are identical for all tasks, making it
easier for us to run experiments on a range of tasks using the same code.

Since we are comparing two approaches that are both designed to have a finite
amount of states, we want the state space to be non-continuous. We often refer
to tasks with these kind of state-spaces as /tabular/ tasks. For our
experiments, we selected 13 tabular tasks from the gym library: Copy-v0,
RepeatCopy-v0, ReversedAddition-v0, ReversedAddition3-v0, DuplicatedInput-v0,
Reverse-v0, Blackjack-v0, FrozenLake-v0, FrozenLake8x8-v0, CliffWalking-v0,
NChain-v0, Roulette-v0 and Taxi-v2. Although these tasks differ in their goals,
what's important to remember that all these tasks have a finite amount of
states, a finite amount of actions, and they all return a reward when a new
state is reached.

One more thing. How can we assure that we are making a fair comparison between
our two techniques?

Which environments/tasks will we use?
- discrete tasks from the GYM collection

How can we ensure that we are making a fair comparison?
- what are hyperparameters
- hyperparameter optimization

- random runs over different seeds using optimal parameters.



* Results

* Conclusion
