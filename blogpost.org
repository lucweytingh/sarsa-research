#+BIND: org-export-use-babel nil
#+TITLE: A comparison between regular and Expected SARSA
#+AUTHOR: Luc Weytingh
#+EMAIL: <lucweytingh321@gmail.com>
#+DATE: October 11, 2021
#+LATEX: \setlength\parindent{0pt}
#+LaTeX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage[margin=0.8in]{geometry}
#+LATEX_HEADER_EXTRA:  \usepackage{mdframed}
#+LATEX_HEADER_EXTRA: \BeforeBeginEnvironment{minted}{\begin{mdframed}}
#+LATEX_HEADER_EXTRA: \AfterEndEnvironment{minted}{\end{mdframed}}
#+MACRO: NEWLINE @@latex:\\@@ @@html:<br>@@
#+PROPERTY: header-args :exports both :session blogpost :cache :results value
#+OPTIONS: ^:nil
#+LATEX_COMPILER: pdflatex

* Introduction

Humans -- and animals -- do a great deal of learning throughout their
life. When we move to a new city, where we have never been before, we find
ourselves in an unknown environment. In order to be able to do the things we
want to, we need to learn things about the environment; for instance, /"Where
is the supermarket?"/ The best way to learn such things is to interact with the
environment, experience what happens as you do so, and learn as much as you can
from those experiences! To find a supermarket in that new city, the easiest
thing is to walk through the streets, and when you find one, remember its
location.[fn:: Well, before the internet, at least...] And the more often we
repeat these experiences, the better we learn their outcomes! If we flip a coin
and it lands on heads everytime, the more times it lands heads, the more
certain we become that something is funky with the coin. In other words, the
more often we experience a certain outcome of some action or event, the
stronger our beliefs about that outcome become. The repetition of those
experiences *reinforce* our beliefs about the world. The learning through
repetition of experiences is what we call *Reinforcement Learning* (RL), and we
do it all the time.

Nowadays, it's not just humans and animals who apply Reinforcement Learning,
but computers can do it as well. This can be very useful: for instance, if we
want some (computer-controlled) robots to do a dangerous (or boring!) activity
so that we don't have to, those robots should know how to perform that
activity. We could try to pre-program the computers so that we tell them what
to do in every possible scenario, but this is very tricky, especially if the
environment in which they act is dynamic and unpredictable. Instead, what if we
would program the computer to learn through reinforcement, and then send a
couple pioneer robots out into the environment to collect experiences? That
way, we are sure that, eventually, they can learn from every situation they
encounter (at the cost, perhaps, of a couple robots). Computers could learn how
to assemble cars, how to mine for precious metals, how to play (video)games,
and so on! <<some more examples?>>

Computers can not experience the world in as much detail and nuance as we
humans do <<TODO: WHY?>>, and so, they need some simplified version of the
world to work with. In RL, researchers use three concepts to achieve this
simplification: states, actions, and rewards.

A state (denoted with $S$) is simply a complete description of (the relevant
parts of) the world in which the computer needs to act. If the computer
controls a robot which can walk around, the state might consist of the current
position of the robot, for instance. If the computer is learning to play a
videogame, the state is often just: the current screen. If the computer
controls an airplane, the state would include all kinds of stuff like: the
weight of the plane, the position in space, whether there's clouds around,
whether the wheels are down or not -- basically, all the things of which we,
the programmer of the computer, think can be relevant when making decisions
(without any superfluous information).

An action (denoted with $A$) is simply an action that a computer can take from
any state. If the computer is controlling that robot which can walk around,
then its actions might be to move forward, or to the left, or to the right, or
backwards (or to stand still).

A reward (denoted with $R$) is any number, either negative or positive, which
is rewarded after an action $A$ is taken from a specific state $S$. If we want
the computer to find the treasure in a maze, then the action taken which
directly exposes the treasure might have a very large reward. For instance,
turning left ($A$) at a certain point in the maze ($S$) might have a reward
$R(S, A) = 100$. When the computer is playing a videogame in which the
challenge is to survive as long as possible, then every action taken through
which the player does not die would have a positive reward.

In this blog post we will only discuss /episodic tasks/, which are tasks which
have some kind of end. If the task is to escape a maze, then one episode would
start at the moment in which the computer is 'dropped' into the maze, and would
end when the exit has been reached. To reach the end of an episode, the
computer has to decide to take a sequence of actions. These actions each have a
reward, and so you can /score/ the overall episode, simply by summing the
reward. When the goal is for the computer to escape as quickly as possible,
every action in which it has not escaped yet might have a small negative
reward. Then, episodes at which the computer takes more actions (time) to
escape the maze have a lower score.

In the study of Artificial Intelligence (AI), many efforts have been made to
try and make it possible for RL to be performed by computers, and the field of
RL has a long history. In (almost all of) this research, the problem boils down
to finding some algorithm which can perform the following task: based on a
bunch of experiences, what is the best way for a computer to learn the optimal
strategy? Here, the higher the average score a computer gets when applying the
strategy in the world, the better we say that strategy is.

<<maybe we can put this in a nice special info box>>
By the way, as humans, we also learn in another way, which we are really good
at: we learn from others. For computers, this is not impossible to accomplish,
but we always need RL for computers keep learning from their experiences. So,
we better get good at it!

* Introduction OLD :noexport:

Methods based on Temporal Difference (TD) learning form a central and novel
role within Reinforcement Learning (RL). A big advantage of TD methods over
earlier approaches is that they work in an online setting with no previous
knowledge of the environment dynamics.

Two main approaches can be identified within the TD methods: on-policy and
off-policy methods. On-policy methods are generally simpler methods. They
generate behaviour and update their action-value estimates using the same
policy. To ensure exploration, a random action is chosen according to some
probability $\epsilon$. Due to this random selection of actions, the learned
action-value estimates are not for the optimal policy, but for a near-optimal
policy that explores randomly. Off-policy methods split the behaviour
generation and action-value updates into two, using a target policy to learn
the optimal policy and a behaviour policy to generate behaviour. This generally
results in a more powerful and general policy, at the cost of slower
convergence [TODO: REFERENCE to RL book].

In this blogpost, we compare two TD methods to confirm this [TODO: nagaan
of this goed terugrefereert] theoretical trade-off between compute time and
sample efficiency: SARSA and Expected SARSA. The former uses an on-policy
approach and the latter uses an off-policy approach. [OPTIONAL] Expected SARSA
is very similar to a more widely known and used off-policy TD-method called
Q-learning. The difference lies within the generation of the actions: while
Q-learning takes the maximum valued action to generate behaviour, Expected
SARSA uses the expected value of each action, thereby taking into account the
probability of each action under the current policy. [/OPTIONAL]

We investigate the differences in several non-continuous environments from the
open source gym library [TODO: reference], reporting on the differences in
 compute-time and sample-efficiency and their origin.


# HYPOTHESE

** Hypothesis
* Method
** Hyperparameters

* Results

* Conclusion
